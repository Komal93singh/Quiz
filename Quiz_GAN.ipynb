{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIZ : GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is the primary role of the discriminator in a GAN?\n",
    "1. Generate new data\n",
    "2. Distinguish between real and fake data\n",
    "3. Minimize the generator's loss\n",
    "4. Cluster similar data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **1. Generate new data**\n",
    "### Explanation : Distinguish between real and fake data The discriminator’s main task is to classify data as real or fake, helping to guide the generator’s improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. which of the following types of GANs is specifically designed to prevent mode collapse?\n",
    "1. DCGAN\n",
    "2. Conditional GAN\n",
    "3. WGAN\n",
    "4. BigGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **3. WGAN**\n",
    "### Explanation : WGAN Wasserstein GAN (WGAN) uses the Wasserstein distance to help reduce mode collapse, which occurs when the generator produces limited diversity in outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. In GAN training, the generator aims to minimize the:\n",
    "1. Generator loss\n",
    "2. Discriminator loss\n",
    "3. Wasserstein distance\n",
    "4. Reconstruction loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **1. Generator loss**\n",
    "### Generator loss The generator seeks to minimize its own loss by generating data that the discriminator misclassifies as real.\n",
    "In GAN training, the generator aims to minimize its own loss (generator loss). This loss is a measure of how well the generator’s output can fool the discriminator into thinking the generated data is real. The generator improves by minimizing this loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Which types of GAN uses a deeper architecture with convolutional layers to enhance stability during training?\n",
    "1. StyleGAN\n",
    "2. CGAN\n",
    "3. CycleGAN\n",
    "4. DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **4. DCGAN**\n",
    "### Explanation : DCGAN Deep Convolutional GANs (DCGANs) incorporate convolutional layers to stabilize training and generate high-quality images.\n",
    "DCGAN uses a deeper architecture with convolutional layers for both the generator and discriminator, which helps enhance stability during training. Convolutional layers improve feature extraction and make the model more stable compared to traditional fully connected networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. What is mode collapse in GANs?\n",
    "1. when the generator produces outputs with low diversity\n",
    "2. when the discriminator fails to classify any samples correctly\n",
    "3. When both generator and discriminator losses become zero\n",
    "4. when the GAN converges too slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct option is : **1. when the generator produces outputs with low diversity**\n",
    "### Explanation : When the generator produces outputs with low diversity Mode collapse is a common GAN issue where the generator produces a limited variety of outputs, often resulting in repetitive samples.\n",
    "Mode collapse occurs when the generator produces a limited variety of outputs, mapping many different inputs to the same or similar output, resulting in low diversity in the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. What types of GAN is primarily used for high-resolution image synthesis?\n",
    "1. BigGAN\n",
    "2. WGAN\n",
    "3. DCGAN\n",
    "4. StyleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **4. StyleGAN**\n",
    "### Explanation: StyleGAN is designed to create high-resolution and highly realistic images, often used in face generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Which function is typically used as the loss function for the discriminator in a traditional GAN?\n",
    "1. Mean Squared error\n",
    "2. Binary cross-entropy\n",
    "3. Wasserstein distance\n",
    "4. categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. Binary cross-entropy**\n",
    "### Explanation : Binary cross-entropy Binary cross-entropy is often used to help the discriminator distinguish between real and fake samples in GANs.\n",
    "In traditional GANs, the discriminator uses binary cross-entropy as its loss function. It classifies each input as either real (label 1) or fake (label 0), and binary cross-entropy measures the error in these classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. In a Conditional GAN (CGAN), the generator and discriminator receive additional input information such as:\n",
    "1. Random noise only\n",
    "2. Label information\n",
    "3. VAlidation data\n",
    "4. Image dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. Label information**\n",
    "### Explanation : Label information CGANs use conditional data (like labels) to produce outputs aligned with specific classes or conditions.\n",
    "In a Conditional GAN (CGAN), both the generator and discriminator receive additional input in the form of label information (e.g., class labels), which conditions the generation process and allows the model to generate data based on specific attributes or categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Why is gradient penalty used in WGAN-GP?\n",
    "1. To improve image quality \n",
    "2. To stabilize GAN training\n",
    "3. To enforce discriminator accuracy\n",
    "4. To minimize overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer : **1. To improve image quality** \n",
    "### Explanation :To stabilize GAN training Gradient penalty helps stabilize the training of WGANs by constraining the gradient norm, reducing the risk of oscillatory behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10.What role does the Conv2DTranspose layer play in the generator of a GAN?\n",
    "1. It downscales images\n",
    "2. It upsamples images\n",
    "3. It applies dropout\n",
    "4. It reduces image dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. It upsamples images**\n",
    "### Explanation : It upsamples images Conv2DTranspose layers are commonly used in generators to increase image resolution by performing upsampling.\n",
    "The Conv2DTranspose layer (also known as a deconvolution layer) is used in the generator of a GAN to upsample images, increasing their spatial resolution from low-dimensional data to higher-resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. Which  metrics does WGAN aim to optimize to improve GAN stability?\n",
    "1. KL divergence\n",
    "2. Cross-entropy loss\n",
    "3. Wasserstein distance\n",
    "4. L2 distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **3. Wasserstein distance**\n",
    "### Explanation : Wasserstein distance WGANs use Wasserstein distance instead of cross-entropy, offering smoother gradients and improved stability.\n",
    "WGAN optimizes the Wasserstein distance (also known as Earth Mover’s distance) between the real and generated data distributions. This metric provides a smoother and more stable training process compared to traditional GAN loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. In GANs, mode collapse can be a problem because :\n",
    "1. It leads to higher computational cost\n",
    "2. It limits the generator's diversity in output\n",
    "3. It improves the generator's performance\n",
    "4. It reduces discriminator accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. It limits the generator's diversity in output**\n",
    "### Explanation : It limits the generator's diversity in output Mode collapse results in repetitive outputs from the generator, reducing the diversity of generated data.\n",
    "Mode collapse occurs when the generator produces a limited variety of outputs, reducing the diversity of generated data, which undermines the overall performance of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. Which components of GANs is updated by real and fake samples during training?\n",
    "1. Discriminator only\n",
    "2. Generator only\n",
    "3. Both Generator and discriminator\n",
    "4. Encoder only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **1. Discriminator only**\n",
    "### Explanation : Discriminator only The discriminator is updated by training on both real and generated (fake) samples to classify them accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. What architectural features makes StyleGAN uniques compared to other GANs?\n",
    "1. The generator improves rapidly\n",
    "2. The generator stops learning\n",
    "3. The GAN's loss function converges\n",
    "4. Mode collapse occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. The generator stops learning**\n",
    "### Explanation : Use of style-based latent space StyleGAN incorporates a unique style-based latent space to generate highly detailed and controlled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. In a DCGAN, which of the following activation functions is commonly used for the last layer of the generator?\n",
    "1. ReLU\n",
    "2. Sigmoid\n",
    "3. Softmax\n",
    "4. Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **4. Tanh**\n",
    "### Explanation: Tanh Tanh is often used in the generator’s output layer for DCGANs to generate values in a specific range, usually between -1 and 1.\n",
    "In a DCGAN, the tanh activation function is commonly used for the last layer of the generator to output values in the range of -1 to 1, which is suitable for generating images where pixel values are normalized to this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. Which of the following GANs is best suited for translating images between domains, such as from photos to paintings?\n",
    "1. DCGAN\n",
    "2. CycleGAN\n",
    "3. WGAN\n",
    "4. BigGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. CycleGAN**\n",
    "### Explanation : CycleGAN CycleGAN is designed for image-to-image translation between domains without paired examples.\n",
    " CycleGAN is specifically designed for image-to-image translation tasks, such as translating images between domains (e.g., photos to paintings), without needing paired data. It uses a cycle consistency loss to ensure that the translation is reversible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Why is label smoothing used in GANs?\n",
    "1. To speed up training\n",
    "2. To prevent mode collapse\n",
    "3. To reduce overconfidence in the discriminator \n",
    "4. To decrease generator loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **3. To reduce overconfidence in the discriminator**\n",
    "### Explanation : To reduce overconfidence in the discriminator Label smoothing can prevent the discriminator from becoming too confident, which improves GAN stability.\n",
    "Label smoothing is used in GANs to reduce overconfidence in the discriminator by modifying the target labels from 1 (real) to slightly less than 1 (e.g., 0.9). This helps the discriminator avoid being overly confident, leading to better training dynamics and improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18. Which of the following is a disadvantage of training GANs?\n",
    "1. Fast convergence\n",
    "2. High computational cost\n",
    "3. Easily tuned hyperparameters\n",
    "4. Simple model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. High computational cost**\n",
    "### Explanation: High computational cost GANs can be computationally expensive due to complex networks and iterative training between generator and discriminator.\n",
    "Training GANs can be computationally expensive due to the need for generating large amounts of data and updating both the generator and discriminator. This can lead to high memory and processing requirements, especially with complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q19. What loss function is used in a Wasserstein GAN (WGAN)?\n",
    "1. Binary cross-entropy\n",
    "2. Mean squared error\n",
    "3. Hinge loss\n",
    "4. Wasserstein loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **4. Wasserstein loss**\n",
    "### Explanation : Wasserstein loss Wasserstein GANs use the Wasserstein loss to achieve stable convergence and reduce mode collapse.\n",
    " WGAN uses Wasserstein loss (also known as Earth Mover’s distance) to measure the difference between the real and generated data distributions, providing a more stable and informative training signal compared to binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q20. What is the main challenge in training GANs?\n",
    "1. optimizing both networks simultaneously\n",
    "2. Lack of training data\n",
    "3. Poor initialization of weights\n",
    "4. Overfitting of the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct option is : **1. optimizing both networks simultaneously**\n",
    "### Explanation: Optimizing both networks simultaneously The challenge in training GANs is to balance the learning of both the generator and discriminator, as one may overpower the other.\n",
    "The main challenge in training GANs is optimizing both the generator and discriminator simultaneously. The two networks are adversarial, and balancing their training to ensure neither overpowers the other can be difficult, often leading to instability or mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q21. Which architecture is most often used for generating high-quality images from random noise in GANs?\n",
    "1. fully connected networks\n",
    "2. Recurrent networks\n",
    "3. Convolutional neural networks (CNNs)\n",
    "4. Residual networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **3. Convolutional neural networks (CNNs)**\n",
    "### Explanation: Convolutional neural networks (CNNs) CNNs are highly effective for image generation because they preserve spatial hierarchies in images.\n",
    "Convolutional neural networks (CNNs) are commonly used in GANs for generating high-quality images. CNNs are effective at capturing spatial hierarchies in image data, making them ideal for image generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q22. What happens when a GAN's discriminator becomes too accurate?\n",
    "1. The generator improves rapidly\n",
    "2. The generator stops learning\n",
    "3. The GAN's loss function converges\n",
    "4. Mode collapse occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. The generator stops learning**\n",
    "### Explanation : The generator stops learning When the discriminator becomes too accurate, it provides minimal gradient feedback to the generator, slowing or halting its improvement.\n",
    "When the discriminator becomes too accurate, it provides very little gradient for the generator, making it harder for the generator to learn and improve. This can cause the generator to stop learning, hindering the overall training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q23. What techniques in GANs helps improve the quality and diversity of generated images?\n",
    "1. Dropout\n",
    "2. Batch normalization\n",
    "3. Data augmentation\n",
    "4. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **2. Batch normalization**\n",
    "### Explanation : Batch normalization stabilizes the learning process by normalizing activations, leading to better quality and diversity in generated images.\n",
    "Batch normalization helps improve the stability and convergence of GAN training by normalizing activations, which in turn can improve the quality and diversity of generated images. It helps prevent issues like mode collapse and speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q24. In GANs, what is the effect of using a deep generator and discriminator?\n",
    "1. Faster convergence\n",
    "2. Easier model evaluation\n",
    "3. More complex and potentially unstable training\n",
    "4. No impact on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : **3. More complex and potentially unstable training**\n",
    "### Explanation: More complex and potentially unstable training Deeper models increase the difficulty of training, making them prone to instability if not properly tuned.\n",
    "Using a deep generator and discriminator increases the complexity of the model, which can make the training process more difficult and unstable. It requires more careful tuning and can lead to issues like mode collapse or vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q25. Which of the following GAN architectures is designed to create realistic faces?\n",
    "1. CylceGAN\n",
    "2. StyleGAN\n",
    "3. DCGAN\n",
    "4. Conditional GAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is : 2. StyleGAN\n",
    "### Explanation : StyleGAN StyleGAN has been widely used for generating high-quality, realistic human faces with a focus on style-based image generation.\n",
    "StyleGAN is specifically designed to generate highly realistic faces by using a style-based architecture that controls different levels of features (e.g., facial details, shapes, and textures), allowing for high-quality and diverse face generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
